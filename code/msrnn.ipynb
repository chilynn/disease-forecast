{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read climate sequence\n",
    "- palce: hangzhou, zhejiang, China\n",
    "- time: from 20120101 to 20151231\n",
    "- features: morning rainfall, afternoon rainfall, night rainfall, min temperature, max temperature, wind force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climates sequence length: 2918\n"
     ]
    }
   ],
   "source": [
    "climates = []\n",
    "infile = open(\"../data/climate_hangzhou.csv\",\"rb\")\n",
    "jump_header = False\n",
    "for row in infile:\n",
    "    if not jump_header:\n",
    "        jump_header = True\n",
    "        continue\n",
    "    row = row.strip().decode(\"utf-8\")\n",
    "    items = row.split(\",\")\n",
    "    dates = items[0]\n",
    "    rainfall_morning = float(items[1])\n",
    "    rainfall_afternoon = float(items[2])\n",
    "    rainfall_night = float(items[3])\n",
    "    min_temperature = float(items[4])\n",
    "    max_temperature = float(items[5])\n",
    "    wind_forece = float(items[6])\n",
    "    climates.append([rainfall_afternoon,min_temperature,max_temperature,wind_forece])\n",
    "#copy another four years datas\n",
    "climates += climates\n",
    "print \"climates sequence length: \"+str(len(climates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read patient visit records\n",
    "- records come from a hangzhou hospital\n",
    "- each row contains patient id, diagnose code(icd9), diagnose time (for example: 0,R10.4,2013-01-15 11:32:14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient size: 141007\n"
     ]
    }
   ],
   "source": [
    "patient_to_records = {}\n",
    "disease_set = set()\n",
    "disease_to_frequency = {}\n",
    "infile = open(\"../data/patient_visit_record.csv\",\"rb\")\n",
    "for row in infile:    \n",
    "    row = row.strip().decode('utf-8')\n",
    "    items = row.split(',')\n",
    "    if len(items) != 3: continue\n",
    "    pid = int(items[0])\n",
    "    disease = items[1]\n",
    "    visit_date_time = items[2]\n",
    "    #unknown data format exception\n",
    "    if u\"0001-01-01\" in visit_date_time:\n",
    "        continue\n",
    "    visit_date = visit_date_time.split()[0]\n",
    "    visit_time = visit_date_time.split()[1]\n",
    "    visit_date_format = time.strptime(visit_date, \"%Y-%m-%d\")\n",
    "    patient_to_records.setdefault(pid,{}).setdefault(visit_date_format,[]).append(disease)\n",
    "    disease_set.add(disease)\n",
    "    disease_to_frequency.setdefault(disease,0)\n",
    "    disease_to_frequency[disease] += 1\n",
    "print \"patient size: \"+str(len(patient_to_records.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create index for disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease size: 4006\n",
      "R10.4 22864\n",
      "K29.7 22462\n",
      "I10.x 21398\n",
      "R05.x 19241\n",
      "J06.9 18573\n"
     ]
    }
   ],
   "source": [
    "disease_to_index = {}\n",
    "index_to_disease = {}\n",
    "disease_index = 0\n",
    "for disease in disease_set:\n",
    "    disease_to_index[disease] = disease_index\n",
    "    index_to_disease[disease_index] = disease\n",
    "    disease_index += 1\n",
    "disease_size = len(disease_to_index.keys())\n",
    "print \"disease size: \"+str(disease_size)\n",
    "disease_frequency_sorted = sorted(disease_to_frequency.items(),key=lambda p:p[1],reverse=True)\n",
    "for t in disease_frequency_sorted[:5]:\n",
    "    print t[0]+\" \"+str(t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datas and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datas = []\n",
    "targets = []\n",
    "MIN_VISIT_NUM = 5\n",
    "#one interval_unit represents k days, here we define k = 7 which is a week\n",
    "interval_unit = 7 \n",
    "for patient in patient_to_records.keys():\n",
    "    records = []\n",
    "    records_sorted = sorted(patient_to_records[patient].items(),key=lambda p:p[0],reverse=False)\n",
    "    pre_visit_date = None\n",
    "    for record in records_sorted:\n",
    "        visit_date = record[0]\n",
    "        visit_day_index = (visit_date[0] - 2012)*365 + (visit_date[7])\n",
    "        diseases = record[1]\n",
    "        disease_index_list = [disease_to_index[disease] for disease in diseases] \n",
    "        visit_index_unitize = int(visit_day_index / interval_unit)\n",
    "        records.append([visit_index_unitize]+disease_index_list)\n",
    "        pre_visit_date = visit_date\n",
    "    if len(records) < MIN_VISIT_NUM: continue\n",
    "    datas.append(records[:-1])\n",
    "    targets.append(records[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 79916\n",
      "test size: 8880\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(datas, targets, test_size=0.1, random_state=0)\n",
    "print \"train size: \"+str(len(X_train))\n",
    "print \"test size: \"+str(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define RNN\n",
    "class RNNNumpy:\n",
    "    def __init__(self, feature_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.input_dim = feature_dim\n",
    "        self.output_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #4 feature each day and consider the current week and previous week, totally 7*2 days\n",
    "        self.climate_input_dim = 4*7*2\n",
    "        self.climate_hidden_dim = 10\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.predict_n_unit = 52\n",
    "        #predict top k diseases for a patient\n",
    "        self.k = 10\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./self.input_dim), np.sqrt(1./self.input_dim), (hidden_dim, self.input_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.Q = np.random.uniform(-np.sqrt(1./self.climate_input_dim), np.sqrt(1./self.climate_input_dim), (self.climate_hidden_dim, self.climate_input_dim))\n",
    "        self.T = np.random.uniform(-np.sqrt(1./self.climate_hidden_dim), np.sqrt(1./self.climate_hidden_dim), (hidden_dim, self.climate_hidden_dim))\n",
    "        self.P = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (self.output_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forward propagation for one sample\n",
    "def forward_propagation(self, x, climates):\n",
    "    T = len(x)\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    c = np.zeros((T, self.predict_n_unit, self.climate_input_dim))\n",
    "    h = np.zeros((T, self.predict_n_unit, self.climate_hidden_dim))\n",
    "    m = np.zeros((T, self.predict_n_unit, self.hidden_dim))\n",
    "    o = np.zeros((T, self.predict_n_unit, self.output_dim))\n",
    "    for t in np.arange(T):\n",
    "        visit_index_unitize = int(x[t][0])\n",
    "        x_input = np.zeros(self.input_dim)\n",
    "        x_input[x[t][1:]] = 1\n",
    "        s[t] = np.tanh(self.U.dot(x_input) + self.W.dot(s[t-1]))\n",
    "        for future_visit_offset in range(self.predict_n_unit):   \n",
    "            c[t][future_visit_offset] = np.array([climates[(visit_index_unitize + future_visit_offset + 1) * 7 - i] for i in range(14)]).ravel()\n",
    "            h[t][future_visit_offset] = np.tanh(self.Q.dot(c[t][future_visit_offset]))\n",
    "            m[t][future_visit_offset] = np.tanh(self.V.dot(s[t]) + self.T.dot(h[t][future_visit_offset]))\n",
    "            o[t][future_visit_offset] = softmax(self.P.dot(m[t][future_visit_offset]))\n",
    "    return [o, m, h, c, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, X, Y):\n",
    "    Loss = 0\n",
    "    # For each sample\n",
    "    for i in np.arange(len(Y)):\n",
    "        o, m, h, c, s = self.forward_propagation(X[i],climates)\n",
    "        for t in range(len(Y[i])):\n",
    "            current_visit_index_unitize = int(X[i][t][0])\n",
    "            future_visit_index_unitize = int(Y[i][t][0])\n",
    "            future_visit_offset = future_visit_index_unitize - current_visit_index_unitize\n",
    "            future_diseases = Y[i][t][1:]\n",
    "            if future_visit_offset >= self.predict_n_unit:\n",
    "                future_visit_offset = self.predict_n_unit - 1\n",
    "            correct_predict_probability = o[t,future_visit_offset,future_diseases]\n",
    "            Loss += -1 * np.sum(np.log(correct_predict_probability))\n",
    "    return Loss\n",
    "\n",
    "def calculate_loss(self, X, Y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in Y))\n",
    "    return self.calculate_total_loss(X,Y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.246454\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(disease_size)\n",
    "print \"Loss: %f\" % model.calculate_loss(X_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, m, h, c, s = self.forward_propagation(x,climates)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    dLdQ = np.zeros(self.Q.shape)\n",
    "    dLdT = np.zeros(self.T.shape)\n",
    "    dLdP = np.zeros(self.P.shape)\n",
    "    \n",
    "    delta_o = o\n",
    "    for t in range(T):\n",
    "        y_sum = len(y[t][1:])\n",
    "        current_visit_index_unitize = int(x[t][0])\n",
    "        future_visit_index_unitize = int(y[t][0])\n",
    "        future_visit_offset = future_visit_index_unitize - current_visit_index_unitize\n",
    "        future_diseases = y[t][1:]\n",
    "        if future_visit_offset >= self.predict_n_unit:\n",
    "            future_visit_offset = self.predict_n_unit - 1\n",
    "        delta_o[t,future_visit_offset,np.arange(self.output_dim)] *= y_sum\n",
    "        delta_o[t,future_visit_offset,future_diseases] -= 1.\n",
    "\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        current_visit_index_unitize = int(x[t][0])\n",
    "        future_visit_index_unitize = int(y[t][0])\n",
    "        future_visit_offset = future_visit_index_unitize - current_visit_index_unitize\n",
    "        if future_visit_offset >= self.predict_n_unit: future_visit_offset = self.predict_n_unit - 1\n",
    "        dLdP += np.outer(delta_o[t][future_visit_offset],m[t][future_visit_offset])   \n",
    "        delta_m = self.P.T.dot(delta_o[t][future_visit_offset]) * (1 - (m[t][future_visit_offset] ** 2))\n",
    "        dLdT += np.outer(delta_m, h[t][future_visit_offset])\n",
    "        dLdV += np.outer(delta_m, s[t])\n",
    "        delta_c = self.T.T.dot(delta_m) * (1 - (h[t][future_visit_offset] ** 2))\n",
    "        dLdQ += np.outer(delta_c,c[t][future_visit_offset])\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_m) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            X = np.zeros(self.input_dim)\n",
    "            X[x[bptt_step][1:]] = 1            \n",
    "            dLdU += np.outer(delta_t, X)\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdP, dLdT, dLdQ, dLdV, dLdU, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "Performing gradient check for parameter P with size 1000.\n",
      "Gradient check for parameter P passed.\n",
      "(10, 10)\n",
      "Performing gradient check for parameter T with size 100.\n",
      "Gradient check for parameter T passed.\n",
      "(10, 56)\n",
      "Performing gradient check for parameter Q with size 560.\n",
      "Gradient check for parameter Q passed.\n",
      "(10, 10)\n",
      "Performing gradient check for parameter V with size 100.\n",
      "Gradient check for parameter V passed.\n",
      "(10, 100)\n",
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "(10, 10)\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:29: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['P','T','Q','V','U','W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print parameter.shape\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([[0,1,2],[3,4,5],[6,7,8]], [[3,4,5],[6,7,8],[9,10,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdP, dLdT, dLdQ, dLdV, dLdU, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.P -= learning_rate * dLdP\n",
    "    self.T -= learning_rate * dLdT\n",
    "    self.Q -= learning_rate * dLdQ\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.05, nepoch=10, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            print \"map: \" + str(meanAveragePrecision(model, X_test[:100], y_test[:100], disease_size))\n",
    "            print \"ar: \"+str(averageRank(model,X_test[:100],y_test[:100],disease_size))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(disease_size)\n",
    "losses = train_with_sgd(model, X_train[:1000], y_train[:1000], nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric\n",
    "- AR(Average Rank)\n",
    "- MAP(Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def averageRank(model, X_test, y_test, top_k=10):\n",
    "    average_rank = 0\n",
    "    counter = 0\n",
    "    for i in range(len(X_test)):\n",
    "#         if (i % 2000 == 0): print \"current handling at: \"+str(i)\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        pre_visit_index_unitize = x[-1][0]\n",
    "        last_visit_index_unitize = y[-1][0]\n",
    "        last_diseases = y[-1][1:]\n",
    "        o, m, h, c, s = model.forward_propagation(x,climates)\n",
    "        last_visit_offset = last_visit_index_unitize - pre_visit_index_unitize\n",
    "        if last_visit_offset >= model.predict_n_unit:\n",
    "            last_visit_offset = model.predict_n_unit - 1\n",
    "        predict = list(o[-1][last_visit_offset].argsort()[-top_k:][::-1])\n",
    "        predict_baseline = [disease_to_index[val[0]] for val in disease_frequency_sorted]\n",
    "        for disease_index in last_diseases:\n",
    "            average_rank += predict.index(disease_index)+1\n",
    "            counter += 1\n",
    "    return 1.0*average_rank/counter      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def meanAveragePrecision(model, X_test, y_test, top_k=10):\n",
    "    mean_average_precision = 0\n",
    "    for i in range(len(X_test)):\n",
    "#         if (i % 2000 == 0): print \"current handling at: \"+str(i)\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        pre_visit_index_unitize = x[-1][0]\n",
    "        last_visit_index_unitize = y[-1][0]\n",
    "        last_diseases = y[-1][1:]\n",
    "        o, m, h, c, s = model.forward_propagation(x,climates)\n",
    "        last_visit_offset = last_visit_index_unitize - pre_visit_index_unitize\n",
    "        if last_visit_offset >= model.predict_n_unit:\n",
    "            last_visit_offset = model.predict_n_unit - 1\n",
    "        predict = list(o[-1][last_visit_offset].argsort()[-top_k:][::-1])\n",
    "        predict_baseline = [disease_to_index[val[0]] for val in disease_frequency_sorted]\n",
    "        ranks = []\n",
    "        mean_average_precision_item = 0\n",
    "        for disease_index in last_diseases:\n",
    "            ranks.append(predict.index(disease_index)+1)\n",
    "        ranks_sorted = sorted(ranks,reverse=False)\n",
    "#         print ranks_sorted\n",
    "        for i in range(len(ranks_sorted)):\n",
    "            mean_average_precision_item += 1.0*(i+1)/ranks_sorted[i]\n",
    "        mean_average_precision_item /= len(ranks_sorted)\n",
    "#         print mean_average_precision_item\n",
    "        mean_average_precision += mean_average_precision_item\n",
    "    return 1.0*mean_average_precision/len(X_test)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print meanAveragePrecision(model,X_test,y_test,disease_size)\n",
    "print averageRank(model,X_test,y_test,disease_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train size = 1000, test size = 13967, iter = 10, iter interval = 4 mins, hidden = 100, ar = 261, map = 0.064\n",
    "#train size = 5000, test size = 13967, iter = 10, iter interval = 30 mins, hidden = 100, ar = 198, map = 0.0667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
