{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,R10.4,2013-01-15 11:32:14\n",
      "0,I09.9,2013-03-05 11:15:50\n",
      "0,J00,2013-03-30 17:06:20\n",
      "0,H40.9,2013-05-06 10:17:46\n",
      "0,N41.9,2013-06-06 11:29:15\n",
      "0,Z00.0,2013-07-03 15:52:13\n",
      "0,M54.5,2013-07-08 08:32:53\n",
      "0,I10,2013-07-09 08:43:55\n",
      "0,E83.5,2013-07-15 10:18:14\n",
      "0,BEZ20,2013-09-02 13:38:43\n"
     ]
    }
   ],
   "source": [
    "# peek data\n",
    "infile = open(\"../data/patient_visit_record.csv\",\"rb\")\n",
    "row_cnt = 0\n",
    "for row in infile:    \n",
    "    print row.strip()\n",
    "    row_cnt += 1\n",
    "    if row_cnt >= 10:\n",
    "        break\n",
    "#pid, disease_id, date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient size: 141007\n"
     ]
    }
   ],
   "source": [
    "patient_to_records = {}\n",
    "disease_set = set()\n",
    "disease_to_frequency = {}\n",
    "infile = open(\"../data/patient_visit_record.csv\",\"rb\")\n",
    "for row in infile:    \n",
    "    row = row.strip().decode('utf-8')\n",
    "    items = row.split(',')\n",
    "    if len(items) != 3: continue\n",
    "    pid = int(items[0])\n",
    "    disease = items[1]\n",
    "    visit_date_time = items[2]\n",
    "    #unknown data format exception\n",
    "    if u\"0001-01-01\" in visit_date_time:\n",
    "        continue\n",
    "    visit_date = visit_date_time.split()[0]\n",
    "    visit_time = visit_date_time.split()[1]\n",
    "    visit_date_format = time.strptime(visit_date, \"%Y-%m-%d\")\n",
    "    patient_to_records.setdefault(pid,{}).setdefault(visit_date_format,[]).append(disease)\n",
    "    disease_set.add(disease)\n",
    "    disease_to_frequency.setdefault(disease,0)\n",
    "    disease_to_frequency[disease] += 1\n",
    "\n",
    "print \"patient size: \"+str(len(patient_to_records.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease size: 4006\n",
      "R10.4 22864\n",
      "K29.7 22462\n",
      "I10.x 21398\n",
      "R05.x 19241\n",
      "J06.9 18573\n"
     ]
    }
   ],
   "source": [
    "disease_to_index = {}\n",
    "index_to_disease = {}\n",
    "disease_index = 0\n",
    "for disease in disease_set:\n",
    "    disease_to_index[disease] = disease_index\n",
    "    index_to_disease[disease_index] = disease\n",
    "    disease_index += 1\n",
    "disease_size = len(disease_to_index.keys())\n",
    "print \"disease size: \"+str(disease_size)\n",
    "disease_frequency_sorted = sorted(disease_to_frequency.items(),key=lambda p:p[1],reverse=True)\n",
    "for t in disease_frequency_sorted[:5]:\n",
    "    print t[0]+\" \"+str(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = []\n",
    "targets = []\n",
    "MIN_VISIT_NUM = 5\n",
    "#one interval_unit represents k days, here we define k = 7 which is a week\n",
    "interval_unit = 7 \n",
    "for patient in patient_to_records.keys():\n",
    "    records = []\n",
    "    records_sorted = sorted(patient_to_records[patient].items(),key=lambda p:p[0],reverse=False)\n",
    "    pre_visit_date = None\n",
    "    for record in records_sorted:\n",
    "        visit_date = record[0]\n",
    "        visit_day_index = (visit_date[0] - 2012)*365 + (visit_date[7])\n",
    "        diseases = record[1]\n",
    "        disease_index_list = [disease_to_index[disease] for disease in diseases] \n",
    "        visit_index_unitize = int(visit_day_index / interval_unit)\n",
    "        records.append([visit_index_unitize]+disease_index_list)\n",
    "        pre_visit_date = visit_date\n",
    "    if len(records) < MIN_VISIT_NUM: continue\n",
    "    datas.append(records[:-1])\n",
    "    targets.append(records[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 79916\n",
      "test size: 8880\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(datas, targets, test_size=0.1, random_state=0)\n",
    "print \"train size: \"+str(len(X_train))\n",
    "print \"test size: \"+str(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self, feature_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./feature_dim), np.sqrt(1./feature_dim), (hidden_dim, feature_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (feature_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    T = len(x)\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    o = np.zeros((T, self.feature_dim))\n",
    "    for t in np.arange(T):\n",
    "        X = np.zeros(self.feature_dim)\n",
    "        X[x[t][1:]] = 1\n",
    "        s[t] = np.tanh(self.U.dot(X) + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictTopK(self, x, k):\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return o[-1].argsort()[-k:][::-1]\n",
    "\n",
    "RNNNumpy.predictTopK = predictTopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, X, Y):\n",
    "    Loss = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(X)):\n",
    "        o, s = self.forward_propagation(X[i])\n",
    "        for t in range(len(Y[i])):\n",
    "            correct_predict_probability = o[t,Y[i][t][1:]]\n",
    "            Loss += -1 * np.sum(np.log(correct_predict_probability))\n",
    "    return Loss\n",
    "\n",
    "def calculate_loss(self, X, Y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in Y))\n",
    "    return self.calculate_total_loss(X,Y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.217793\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(disease_size)\n",
    "print \"Loss: %f\" % model.calculate_loss(X_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    \n",
    "    delta_o = o\n",
    "    for t in range(T):\n",
    "        y_sum = len(y[t][1:])\n",
    "        delta_o[t,np.arange(self.feature_dim)] *= y_sum\n",
    "        delta_o[t,y[t][1:]] -= 1\n",
    "        \n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])   \n",
    "            X = np.zeros(self.feature_dim)\n",
    "            X[x[bptt_step][1:]] = 1            \n",
    "            dLdU += np.outer(delta_t, X)\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([[0,1,2],[3,4,5],[6,7,8]], [[3,4,5],[6,7,8],[9,10,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.05, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            print \"map: \" + str(meanAveragePrecision(model, X_test[:100], y_test[:100], disease_size))\n",
    "            print \"ar: \"+str(averageRank(model,X_test[:100],y_test[:100],disease_size))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(disease_size)\n",
    "losses = train_with_sgd(model, X_train[:10000], y_train[:10000], nepoch=50, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def averageRank(model, X_test, y_test, top_k=10):\n",
    "    average_rank = 0\n",
    "    counter = 0\n",
    "    for i in range(len(X_test)):\n",
    "#         if (i % 2000 == 0): print \"current handling at: \"+str(i)\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        last_diseases = y[-1][1:]\n",
    "        o, s = model.forward_propagation(x)\n",
    "        predict = list(model.predictTopK(x, top_k))\n",
    "        predict_baseline = [disease_to_index[val[0]] for val in disease_frequency_sorted]\n",
    "        for disease_index in last_diseases:\n",
    "            average_rank += predict.index(disease_index)\n",
    "            counter += 1\n",
    "    return 1.0*average_rank/counter   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def meanAveragePrecision(model, X_test, y_test, top_k=10):\n",
    "    mean_average_precision = 0\n",
    "    for i in range(len(X_test)):\n",
    "#         if (i % 2000 == 0): print \"current handling at: \"+str(i)\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        last_diseases = y[-1][1:]\n",
    "        o, s = model.forward_propagation(x)\n",
    "        predict = list(model.predictTopK(x, top_k))\n",
    "        predict_baseline = [disease_to_index[val[0]] for val in disease_frequency_sorted]\n",
    "        ranks = []\n",
    "        mean_average_precision_item = 0\n",
    "        for disease_index in last_diseases:\n",
    "            ranks.append(predict.index(disease_index)+1)\n",
    "        ranks_sorted = sorted(ranks,reverse=False)\n",
    "        for i in range(len(ranks_sorted)):\n",
    "            mean_average_precision_item += 1.0*(i+1)/ranks_sorted[i]\n",
    "        mean_average_precision_item /= len(ranks_sorted)\n",
    "        mean_average_precision += mean_average_precision_item\n",
    "    return 1.0*mean_average_precision/len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print meanAveragePrecision(model,X_test,y_test,disease_size)\n",
    "print averageRank(model,X_test,y_test,disease_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train size = 10000, test size = 13967, iter = 20, hidden = 100, ar = 172, map = 0.10908\n",
    "#train size = 1000, test size = 13967, iter = 100, learning_rate = 0.005, hidden = 100, ar = 364, map = 0.06268\n",
    "#train size = 1000, test size = 13967, iter = 200, learning_rate = 0.05, hidden = 200, ar = 521, map = 0.050430"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
