{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn import cross_validation\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read climate sequence\n",
    "- palce: hangzhou, zhejiang, China\n",
    "- time: from 20120101 to 20151231\n",
    "- features: morning rainfall, afternoon rainfall, night rainfall, min temperature, max temperature, wind force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climates sequence length: 2918\n"
     ]
    }
   ],
   "source": [
    "climates = []\n",
    "infile = open(\"../data/climate_hangzhou.csv\",\"rb\")\n",
    "jump_header = False\n",
    "for row in infile:\n",
    "    if not jump_header:\n",
    "        jump_header = True\n",
    "        continue\n",
    "    row = row.strip().decode(\"utf-8\")\n",
    "    items = row.split(\",\")\n",
    "    dates = items[0]\n",
    "    rainfall_morning = float(items[1])\n",
    "    rainfall_afternoon = float(items[2])\n",
    "    rainfall_night = float(items[3])\n",
    "    min_temperature = float(items[4])\n",
    "    max_temperature = float(items[5])\n",
    "    wind_forece = float(items[6])\n",
    "    climates.append([rainfall_afternoon,min_temperature,max_temperature,wind_forece])\n",
    "#copy another four years datas\n",
    "climates += climates\n",
    "print \"climates sequence length: \"+str(len(climates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read patient visit records\n",
    "- records come from a hangzhou hospital\n",
    "- each row contains patient id, diagnose code(icd9), diagnose time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient size: 141007\n"
     ]
    }
   ],
   "source": [
    "patient_to_records = {}\n",
    "disease_set = set()\n",
    "disease_to_frequency = {}\n",
    "infile = open(\"../data/patient_visit_record.csv\",\"rb\")\n",
    "for row in infile:    \n",
    "    row = row.strip().decode('utf-8')\n",
    "    items = row.split(',')\n",
    "    if len(items) != 3: continue\n",
    "    pid = int(items[0])\n",
    "    disease = items[1]\n",
    "    visit_date_time = items[2]\n",
    "    #unknown data format exception\n",
    "    if u\"0001-01-01\" in visit_date_time:\n",
    "        continue\n",
    "    visit_date = visit_date_time.split()[0]\n",
    "    visit_time = visit_date_time.split()[1]\n",
    "    visit_date_format = time.strptime(visit_date, \"%Y-%m-%d\")\n",
    "    tmp = patient_to_records.setdefault(pid,{}).setdefault(visit_date_format,disease)\n",
    "    if tmp != disease:\n",
    "        continue\n",
    "    disease_set.add(disease)\n",
    "    disease_to_frequency.setdefault(disease,0)\n",
    "    disease_to_frequency[disease] += 1\n",
    "print \"patient size: \"+str(len(patient_to_records.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create index for disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease size: 3739\n",
      "R10.4 19495\n",
      "K29.7 16740\n",
      "R05.x 15955\n",
      "J06.9 15163\n",
      "I10.x 14827\n"
     ]
    }
   ],
   "source": [
    "disease_to_index = {}\n",
    "index_to_disease = {}\n",
    "disease_index = 0\n",
    "for disease in disease_set:\n",
    "    disease_to_index[disease] = disease_index\n",
    "    index_to_disease[disease_index] = disease\n",
    "    disease_index += 1\n",
    "disease_size = len(disease_to_index.keys())\n",
    "print \"disease size: \"+str(disease_size)\n",
    "disease_frequency_sorted = sorted(disease_to_frequency.items(),key=lambda p:p[1],reverse=True)\n",
    "for t in disease_frequency_sorted[:5]:\n",
    "    print t[0]+\" \"+str(t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus\n",
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/gensim/models/word2vec.py:651: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "print 'reading corpus'\n",
    "corpus = []\n",
    "for patient in patient_to_records.keys():\n",
    "    records_sorted = sorted(patient_to_records[patient].items(),key=lambda p:p[0],reverse=False)\n",
    "    disease_sequence = []\n",
    "    for record in records_sorted:\n",
    "        disease_sequence.append(str(disease_to_index[record[1]]))\n",
    "    corpus.append(disease_sequence)\n",
    "print 'training'\n",
    "w2v_model = Word2Vec(corpus, size=100, window=15, min_count=0, workers=4)\n",
    "w2v_model.save('../data/word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datas and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = []\n",
    "targets = []\n",
    "MIN_VISIT_NUM = 5\n",
    "#one interval_unit represents k days, here we define k = 7 which is a week\n",
    "interval_unit = 30\n",
    "for patient in patient_to_records.keys():\n",
    "    records = []\n",
    "    records_sorted = sorted(patient_to_records[patient].items(),key=lambda p:p[0],reverse=False)\n",
    "    pre_visit_date = None\n",
    "    for record in records_sorted:\n",
    "        visit_date = record[0]\n",
    "        visit_day_index = (visit_date[0] - 2012)*365 + (visit_date[7])\n",
    "        visit_index_unitize = int(visit_day_index / interval_unit)\n",
    "        records.append([visit_index_unitize]+[disease_to_index[record[1]]])\n",
    "        pre_visit_date = visit_date\n",
    "    if len(records) < MIN_VISIT_NUM: continue\n",
    "    datas.append(records[:-1])\n",
    "    targets.append(records[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 79916\n",
      "test size: 8880\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(datas, targets, test_size=0.1, random_state=0)\n",
    "print \"train size: \"+str(len(X_train))\n",
    "print \"test size: \"+str(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define RNN\n",
    "class RNNNumpy:\n",
    "    def __init__(self, feature_dim, w2v_model, hidden_dim=64, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.input_dim = 100\n",
    "        self.output_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #4 feature each day and consider the current week and previous week, totally 7*2 days\n",
    "        self.climate_input_dim = 4*30*2\n",
    "        self.climate_hidden_dim = 10\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.predict_n_unit = 12\n",
    "        #predict top k diseases for a patient\n",
    "        self.k = 10\n",
    "        #word to vector\n",
    "        self.w2v_model = w2v_model\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./self.input_dim), np.sqrt(1./self.input_dim), (hidden_dim, self.input_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.Q = np.random.uniform(-np.sqrt(1./self.climate_input_dim), np.sqrt(1./self.climate_input_dim), (self.climate_hidden_dim, self.climate_input_dim))\n",
    "        self.T = np.random.uniform(-np.sqrt(1./self.climate_hidden_dim), np.sqrt(1./self.climate_hidden_dim), (hidden_dim, self.climate_hidden_dim))\n",
    "        self.P = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (self.output_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forward propagation for one sample\n",
    "def forward_propagation(self, x, climates):\n",
    "    T = len(x)\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    c = np.zeros((T, self.predict_n_unit, self.climate_input_dim))\n",
    "    h = np.zeros((T, self.predict_n_unit, self.climate_hidden_dim))\n",
    "    m = np.zeros((T, self.predict_n_unit, self.hidden_dim))\n",
    "    o = np.zeros((T, self.predict_n_unit, self.output_dim))\n",
    "    for t in np.arange(T):\n",
    "        visit_index_unitize = int(x[t][0])\n",
    "        x_t = self.w2v_model[str(x[t][1])]\n",
    "        s[t] = np.tanh(self.U.dot(x_t) + self.W.dot(s[t-1]))\n",
    "        for future_visit_offset in range(self.predict_n_unit):   \n",
    "            c[t][future_visit_offset] = np.array([climates[(visit_index_unitize + future_visit_offset + 1) * 30 - i] for i in range(60)]).ravel()\n",
    "            h[t][future_visit_offset] = np.tanh(self.Q.dot(c[t][future_visit_offset]))\n",
    "            m[t][future_visit_offset] = np.tanh(self.V.dot(s[t]) + self.T.dot(h[t][future_visit_offset]))\n",
    "            o[t][future_visit_offset] = softmax(self.P.dot(m[t][future_visit_offset]))\n",
    "    return [o, m, h, c, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, X, Y):\n",
    "    Loss = 0\n",
    "    # For each sample\n",
    "    for i in np.arange(len(Y)):\n",
    "        o, m, h, c, s = self.forward_propagation(X[i],climates)\n",
    "        for t in range(len(Y[i])):\n",
    "            current_visit_index_unitize = int(X[i][t][0])\n",
    "            future_visit_index_unitize = int(Y[i][t][0])\n",
    "            future_visit_offset = future_visit_index_unitize - current_visit_index_unitize\n",
    "            future_diseases = Y[i][t][1:]\n",
    "            if future_visit_offset >= self.predict_n_unit:\n",
    "                future_visit_offset = self.predict_n_unit - 1\n",
    "            correct_predict_probability = o[t,future_visit_offset,future_diseases]\n",
    "            Loss += -1 * np.sum(np.log(correct_predict_probability))\n",
    "    return Loss\n",
    "\n",
    "def calculate_loss(self, X, Y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in Y))\n",
    "    return self.calculate_total_loss(X,Y,)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (56) into shape (240)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b0dbd803b3bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisease_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Loss: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-5827e5226ed6>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Divide the total loss by the number of training examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mRNNNumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_total_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5827e5226ed6>\u001b[0m in \u001b[0;36mcalculate_total_loss\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# For each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mcurrent_visit_index_unitize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a05d41a61dd1>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, x, climates)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture_visit_offset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_n_unit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture_visit_offset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclimates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisit_index_unitize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfuture_visit_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture_visit_offset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture_visit_offset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture_visit_offset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture_visit_offset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (56) into shape (240)"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(disease_size,w2v_model)\n",
    "print \"Loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, m, h, c, s = self.forward_propagation(x,climates)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    dLdQ = np.zeros(self.Q.shape)\n",
    "    dLdT = np.zeros(self.T.shape)\n",
    "    dLdP = np.zeros(self.P.shape)\n",
    "    \n",
    "    delta_o = o\n",
    "    for t in range(T):\n",
    "        y_sum = len(y[t][1:])\n",
    "        current_visit_index_unitize = int(x[t][0])\n",
    "        future_visit_index_unitize = int(y[t][0])\n",
    "        future_visit_offset = future_visit_index_unitize - current_visit_index_unitize\n",
    "        future_diseases = y[t][1:]\n",
    "        if future_visit_offset >= self.predict_n_unit:\n",
    "            future_visit_offset = self.predict_n_unit - 1\n",
    "        delta_o[t,future_visit_offset,np.arange(self.output_dim)] *= y_sum\n",
    "        delta_o[t,future_visit_offset,future_diseases] -= 1.\n",
    "\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        current_visit_index_unitize = int(x[t][0])\n",
    "        future_visit_index_unitize = int(y[t][0])\n",
    "        future_visit_offset = future_visit_index_unitize - current_visit_index_unitize\n",
    "        if future_visit_offset >= self.predict_n_unit: future_visit_offset = self.predict_n_unit - 1\n",
    "        dLdP += np.outer(delta_o[t][future_visit_offset],m[t][future_visit_offset])   \n",
    "        delta_m = self.P.T.dot(delta_o[t][future_visit_offset]) * (1 - (m[t][future_visit_offset] ** 2))\n",
    "        dLdT += np.outer(delta_m, h[t][future_visit_offset])\n",
    "        dLdV += np.outer(delta_m, s[t])\n",
    "        delta_c = self.T.T.dot(delta_m) * (1 - (h[t][future_visit_offset] ** 2))\n",
    "        dLdQ += np.outer(delta_c,c[t][future_visit_offset])\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_m) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "            x_t = self.w2v_model[str(x[bptt_step][1])]\n",
    "            dLdU += np.outer(delta_t, x_t)\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdP, dLdT, dLdQ, dLdV, dLdU, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdP, dLdT, dLdQ, dLdV, dLdU, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.P -= learning_rate * dLdP\n",
    "    self.T -= learning_rate * dLdT\n",
    "    self.Q -= learning_rate * dLdQ\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.05, nepoch=10, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            print \"map: \" + str(meanAveragePrecision(model, X_test[:100], y_test[:100], disease_size))\n",
    "            print \"ar: \"+str(averageRank(model,X_test[:100],y_test[:100],disease_size))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-28 14:10:43: Loss after num_examples_seen=0 epoch=0: 8.245864\n",
      "map: 0.0021487438119\n",
      "ar: 1851.97\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(disease_size,w2v_model)\n",
    "losses = train_with_sgd(model, X_train[:1000], y_train[:1000], nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averageRank(model, X_test, y_test, top_k=10):\n",
    "    average_rank = 0\n",
    "    counter = 0\n",
    "    for i in range(len(X_test)):\n",
    "#         if (i % 2000 == 0): print \"current handling at: \"+str(i)\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        pre_visit_index_unitize = x[-1][0]\n",
    "        last_visit_index_unitize = y[-1][0]\n",
    "        last_diseases = y[-1][1:]\n",
    "        o, m, h, c, s = model.forward_propagation(x,climates)\n",
    "        last_visit_offset = last_visit_index_unitize - pre_visit_index_unitize\n",
    "        if last_visit_offset >= model.predict_n_unit:\n",
    "            last_visit_offset = model.predict_n_unit - 1\n",
    "        predict = list(o[-1][last_visit_offset].argsort()[-top_k:][::-1])\n",
    "        predict_baseline = [disease_to_index[val[0]] for val in disease_frequency_sorted]\n",
    "        for disease_index in last_diseases:\n",
    "            average_rank += predict.index(disease_index)+1\n",
    "            counter += 1\n",
    "    return 1.0*average_rank/counter      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meanAveragePrecision(model, X_test, y_test, top_k=10):\n",
    "    mean_average_precision = 0\n",
    "    for i in range(len(X_test)):\n",
    "#         if (i % 2000 == 0): print \"current handling at: \"+str(i)\n",
    "        x = X_test[i]\n",
    "        y = y_test[i]\n",
    "        pre_visit_index_unitize = x[-1][0]\n",
    "        last_visit_index_unitize = y[-1][0]\n",
    "        last_diseases = y[-1][1:]\n",
    "        o, m, h, c, s = model.forward_propagation(x,climates)\n",
    "        last_visit_offset = last_visit_index_unitize - pre_visit_index_unitize\n",
    "        if last_visit_offset >= model.predict_n_unit:\n",
    "            last_visit_offset = model.predict_n_unit - 1\n",
    "        predict = list(o[-1][last_visit_offset].argsort()[-top_k:][::-1])\n",
    "        predict_baseline = [disease_to_index[val[0]] for val in disease_frequency_sorted]\n",
    "        ranks = []\n",
    "        mean_average_precision_item = 0\n",
    "        for disease_index in last_diseases:\n",
    "            ranks.append(predict.index(disease_index)+1)\n",
    "        ranks_sorted = sorted(ranks,reverse=False)\n",
    "#         print ranks_sorted\n",
    "        for i in range(len(ranks_sorted)):\n",
    "            mean_average_precision_item += 1.0*(i+1)/ranks_sorted[i]\n",
    "        mean_average_precision_item /= len(ranks_sorted)\n",
    "#         print mean_average_precision_item\n",
    "        mean_average_precision += mean_average_precision_item\n",
    "    return 1.0*mean_average_precision/len(X_test)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
